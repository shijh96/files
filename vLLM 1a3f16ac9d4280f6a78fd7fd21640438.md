# vLLM

# vLLM 本地部署 LLM 教程

**vLLM** 是由 UC Berkeley 开发的开源大语言模型推理和服务框架，以高吞吐量和内存高效著称 ([Berkeley vLLM：算力减半、吞吐增十倍- lsgxeva - 博客园](https://www.cnblogs.com/lsgxeva/p/18187514#:~:text=com%2Fvllm,%E5%8F%AF%E4%BB%A5%E6%9C%89%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E9%94%AE%E5%92%8C%E5%80%BC%EF%BC%9B%3B%20%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%EF%BC%9B%3B%20%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84))。

## 1. vLLM 支持的模型类型

vLLM 支持**两大类模型**：**生成式模型（Generative Models）和池化模型（Pooling Models）**。

- **生成式模型**：这类模型用于文本生成（包括对话、续写等）。vLLM 对生成式模型提供一流支持，涵盖大多数常见的大语言模型 ([Generative Models - vLLM](https://docs.vllm.ai/en/v0.7.1/models/generative_models.html#:~:text=Generative%20Models%20,models%20implement%20the%20VllmModelForTextGeneration))。生成式模型通常是自回归Transformer，例如 GPT-2、GPT-NeoX、LLaMA 系列、Falcon、BLOOM 等。vLLM 与 HuggingFace 生态无缝集成，支持众多 HuggingFace 上的开源模型架构，包括 *Aquila*、*Baichuan*、*BLOOM*、*ChatGLM*、*DeciLM*、*Falcon*、*GPT-2*、*GPT-J*、*GPT-NeoX*、*LLaMA* 等 ([Large Language Models: A Survey - arXiv](https://arxiv.org/html/2402.06196v2#:~:text=vLLM%20seamlessly%20supports%20many%20Hugging,BLOOM%2C%20ChatGLM%2C%20DeciLM%2C%20Falcon))。（以上列举的都是目前流行的开源大型模型架构。）
- **池化模型**：这类模型用于生成固定长度的向量表示（embedding）或评分，一般用于嵌入提取、重排序、奖励模型等任务。vLLM 同样支持这些模型 ([vllm/docs/source/models/pooling_models.md at main - GitHub](https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#:~:text=vllm%2Fdocs%2Fsource%2Fmodels%2Fpooling_models.md%20at%20main%20,class%7D%20~vllm))。池化模型可以从生成式模型架构中衍生（例如使用 Transformer 输出CLS向量作为embedding），vLLM 提供了接口 `-task` 来指定模型执行池化任务。如果某个模型既支持生成又支持嵌入等多种任务，可以通过启动参数 `-task` 来切换，例如 `-task embed`（嵌入）、`-task reward`（奖励模型）等 ([vllm/docs/source/models/pooling_models.md at main - GitHub](https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#:~:text=vllm%2Fdocs%2Fsource%2Fmodels%2Fpooling_models.md%20at%20main%20,class%7D%20~vllm))。总的来说，大多数基于 Transformer 的LLM架构（无论是对话生成还是向量嵌入）都可以在 vLLM 上运行。

*注*: **不支持**的模型类型包括需要特殊输入/输出的多模态模型（如纯语音、视频生成等）

## 2. 目前不支持CosyVoice等TTS模型

目前 **vLLM 不支持文本转语音（TTS）模型**。vLLM 专注于文本输入到文本输出的语言模型推理，对需要语音输出的模型并没有直接的支持接口。根据相关讨论，像 CosyVoice、F5 这类语音合成模型通常需要特定的推理流程，并不在 vLLM 等通用 LLM 推理库的支持范围内 ([An Embarrassingly Simple TTS Framework that Everyone Can Touch](https://arxiv.org/html/2412.08237v1#:~:text=An%20Embarrassingly%20Simple%20TTS%20Framework,requiring%20additional%20deployment))。

*原因*: TTS 模型通常不仅仅需要文本输入，还可能涉及音频特征提取、波形生成等复杂流程。这与标准的大语言模型推理有所不同 ([An Embarrassingly Simple TTS Framework that Everyone Can Touch](https://arxiv.org/html/2412.08237v1#:~:text=An%20Embarrassingly%20Simple%20TTS%20Framework,requiring%20additional%20deployment))。

## 3. vLLM 的优势

vLLM 之所以值得使用，本质上在于它针对大模型推理做了多方面的优化，在**速度**和**内存效率**上都具有显著优势。下面总结 vLLM 的几大优点：

- **超高的推理吞吐量**：vLLM 提供**业界领先的服务吞吐量**。实测显示，vLLM 的推理速度比直接使用 HuggingFace Transformers 提高了 *14～24 倍* ([Accelerating AI up to 24x with vLLM: Unleashing the True Potential ...](https://www.aiworks.be/accelerating-ai-up-to-24x-with-vllm-unleashing-the-true-potential-of-local-large-language-models/#:~:text=Accelerating%20AI%20up%20to%2024x,Key%20Takeaways))！即使与专业优化的 HuggingFace TGI (Text Generation Inference) 服务相比，vLLM 仍有大约 *2.2～2.5 倍* 的吞吐提升 ([Accelerating AI up to 24x with vLLM: Unleashing the True Potential ...](https://www.aiworks.be/accelerating-ai-up-to-24x-with-vllm-unleashing-the-true-potential-of-local-large-language-models/#:~:text=Accelerating%20AI%20up%20to%2024x,Key%20Takeaways))。这意味着在相同硬件上，vLLM 每秒可处理的请求数远超其他框架，大大提升了并发性能。
- **高效的内存管理（更低显存占用）**：vLLM 引入了全新的 **PagedAttention** 注意力算法，利用虚拟内存和分页技术来管理注意力的KV缓存。其内存利用率接近最优，内存浪费不到4% ([VLLM介绍原创 - CSDN博客](https://blog.csdn.net/qq_30594137/article/details/137273547#:~:text=))。换言之，vLLM 能将更多的上下文缓存放入显存而几乎无浪费，使得在相同显存下可支持更长的上下文或更多的并发请求。这种内存效率提升使系统能够批量处理更多序列，提高 GPU 利用率，从而**显著提高吞吐量** ([VLLM框架助力高效大模型推理实践 - 百度智能云](https://cloud.baidu.com/article/3364902#:~:text=%E4%B8%80%E3%80%81VLLM%E6%A1%86%E6%9E%B6%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8A%BF%E2%80%8B%20%E8%80%8CVLLM%E6%A1%86%E6%9E%B6%E9%80%9A%E8%BF%87%E4%B8%80%E6%AC%A1%E6%80%A7%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E6%A0%B7%E6%9C%AC%EF%BC%8C%E5%85%85%E5%88%86%E5%88%A9%E7%94%A8%E4%BA%86%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E7%9A%84%E5%B9%B6%E8%A1%8C%E6%80%A7%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E4%BA%86%E9%AB%98%E6%95%88%E7%9A%84%E6%8E%A8%E7%90%86%E6%80%A7%E8%83%BD%E3%80%82%20%E6%AD%A4%E5%A4%96%EF%BC%8CVLLM%E6%A1%86%E6%9E%B6%E8%BF%98%E9%87%87%E7%94%A8%E4%BA%86%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF%20))。
- **动态批处理和调度**：vLLM 的引擎能对传入请求进行**连续动态批处理**。它可以将多个不同长度、不同请求实时地打包在一起做推理，从而充分利用每一次GPU计算 ([VLLM框架助力高效大模型推理实践 - 百度智能云](https://cloud.baidu.com/article/3364902#:~:text=%E4%B8%80%E3%80%81VLLM%E6%A1%86%E6%9E%B6%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8A%BF%E2%80%8B%20%E8%80%8CVLLM%E6%A1%86%E6%9E%B6%E9%80%9A%E8%BF%87%E4%B8%80%E6%AC%A1%E6%80%A7%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E6%A0%B7%E6%9C%AC%EF%BC%8C%E5%85%85%E5%88%86%E5%88%A9%E7%94%A8%E4%BA%86%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E7%9A%84%E5%B9%B6%E8%A1%8C%E6%80%A7%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E4%BA%86%E9%AB%98%E6%95%88%E7%9A%84%E6%8E%A8%E7%90%86%E6%80%A7%E8%83%BD%E3%80%82%20%E6%AD%A4%E5%A4%96%EF%BC%8CVLLM%E6%A1%86%E6%9E%B6%E8%BF%98%E9%87%87%E7%94%A8%E4%BA%86%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF%20))。相比静态批处理，动态批处理无需等待所有请求齐备就绪，能最大化并行度并降低平均延迟。因此在实际服务中，vLLM 能**根据请求情况自动优化计算安排**，开发者无需手工调整批大小。
- **优化的底层实现**：vLLM 内置了针对 Transformer 推理的**高度优化CUDA内核**和计算流程 ([Berkeley vLLM：算力减半、吞吐增十倍- lsgxeva - 博客园](https://www.cnblogs.com/lsgxeva/p/18187514#:~:text=com%2Fvllm,%E5%8F%AF%E4%BB%A5%E6%9C%89%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E9%94%AE%E5%92%8C%E5%80%BC%EF%BC%9B%3B%20%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%EF%BC%9B%3B%20%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84))。许多低级别优化（如张量并行、高效的内存拷贝等）都已经封装在框架中，开箱即用。这些优化进一步降低了推理延迟，并保障高并发时的稳定性。
- **多 GPU 扩展能力**：对于超大模型，vLLM 原生支持 **多GPU的分布式推理**。它实现了 Megatron-LM 样式的张量并行和流水线并行算法，可通过启动参数 `-tensor-parallel-size` 和 `-pipeline-parallel-size` 方便地利用多块 GPU 来加载一个模型 ([Distributed Inference and Serving - vLLM](https://docs.vllm.ai/en/latest/serving/distributed_serving.html#:~:text=,size%204))。例如，可以用 4 块 GPU 来部署 130亿参数模型 OPT-13B ([Distributed Inference and Serving - vLLM](https://docs.vllm.ai/en/latest/serving/distributed_serving.html#:~:text=,size%204))。这种横向扩展能力使 vLLM 能够支持**更大参数量**的模型和**更长上下文**，满足企业级高需求场景。
- **与 HuggingFace 模型生态无缝集成**：vLLM 可以直接加载 HuggingFace Hub 上的绝大多数开源模型，无需修改模型代码 ([Welcome to vLLM!](https://docs.vllm.ai/en/v0.5.2/#:~:text=vLLM%20is%20flexible%20and%20easy,with%20various%20decoding%20algorithms))。只需提供模型名称或本地路径，vLLM 会自动识别并构建相应的模型执行器。这种兼容性让用户可以方便地利用已有的模型权重和Transformer模型实现。此外，vLLM 也支持加载使用 LoRA 等方法微调的模型（可以通过配置在推理时动态应用 LoRA 权重），兼容性强。
- **OpenAI API 接口兼容**：vLLM 内置了一个 **OpenAI-Compatible** 的HTTP服务端。也就是说，您可以用 vLLM 在本地启动一个服务，它提供与 OpenAI API 相同的接口端点（如 `/v1/completions`, `/v1/chat/completions` 等）。这一特性使得将本地部署的模型融入现有应用变得非常简单——任何支持OpenAI API的现有应用或SDK，都可以指向本地的 vLLM 服务而正常工作 ([Quickstart — vLLM](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#:~:text=vLLM%20can%20be%20deployed%20as,You%20can%20specify))。对于想摆脱对OpenAI云服务依赖的用户，vLLM 提供了一个即插即用的替代品。
- **丰富的量化和优化支持**：vLLM 支持直接加载多种**模型量化格式**的权重，如 AWQ、GPTQ、SqueezeLLM 等 ([vLLM - Qwen docs](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#:~:text=Serving%20Quantized%20Models%C2%B6,deploy%20AWQ%20and%20GPTQ))。使用量化模型可以大幅降低显存需求，使得在消费级 GPU 上运行更大的模型成为可能。vLLM 对这些量化模型的原生支持，让用户无需额外转换流程即可部署。一些社区提供的4-bit、8-bit量化权重模型（例如 TheBloke 发布的GPTQ版 LLaMA-2 等）都可以被 vLLM 顺利加载 ([vLLM - Qwen docs](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#:~:text=Serving%20Quantized%20Models%C2%B6,deploy%20AWQ%20and%20GPTQ))。此外，vLLM 还支持 FP16、BF16 等低精度推理，进一步提升性能。

以上优势使得 vLLM 成为本地部署大语言模型的理想选择。简单来说，如果您需要**高性能、本地化**的 LLM 推理服务，vLLM 会是一个很好的方案。

## 4. 本地部署模型的分步指南

下面是一套使用 vLLM 在本地部署 LLM 模型的**步骤指南**。

1. **安装 vLLM**：vLLM 提供 Python 库，通过 pip 安装非常简单。首先确保 Python 版本为3.8~3.11，建议使用 Linux 环境。然后在终端运行安装命令：
    
    ```bash
    pip install vllm
    
    ```
    
    这将从 PyPI 下载并安装最新发布版本的 vLLM ([Installation - vLLM](https://docs.vllm.ai/en/v0.4.0.post1/getting_started/installation.html#:~:text=Install%20vLLM%20with%20CUDA%2012,PyTorch%20release%20versions%20by%20default))。默认情况下，vLLM 已包含预编译的 C++/CUDA 二进制模块（需CUDA 12.1环境） ([Installation - vLLM](https://docs.vllm.ai/en/v0.4.0.post1/getting_started/installation.html#:~:text=Install%20vLLM%20with%20CUDA%2012,PyTorch%20release%20versions%20by%20default))。安装完成后，您可以通过 `python -c "import vllm; print(vllm.__version__)"` 来验证是否成功安装。
    
2. **准备模型权重**：选择您要部署的 LLM 模型。vLLM 可以直接从 Hugging Face Hub 加载模型，只需提供模型名称（或本地路径）。例如，我们以 **ChatGLM2-6B** 模型为例（一个支持中英双语对话的6B参数模型）：
    - 如果您的机器可以联网，您可以在运行时让 vLLM 自动从 Hugging Face 下载权重。准备好模型名称：`THUDM/chatglm2-6b`。
    - 如果需要离线部署，您也可以提前使用 `huggingface-cli` 或 Python 脚本将模型权重下载到本地，然后将 `LLM(model=本地路径)`。下载时请注意模型的体积和所需磁盘空间（例如 ChatGLM2-6B 大约需要 8GB 文件）。
    确保您的**显存/内存**足够容纳所选模型（6B参数FP16大约需要12GB显存）。如果显存不足，可考虑选择更小的模型或使用量化权重版。
3. **加载并测试模型**：使用 vLLM 的 Python 接口加载模型并进行一次简单的推理，以确认部署成功。vLLM 提供了高级的 `LLM` 类用于离线推理。以下是一个最小示例：
    
    ```python
    from vllm import LLM, SamplingParams
    
    # 加载模型和分词器（以 ChatGLM2-6B 为例）
    llm = LLM(model="THUDM/chatglm2-6b", tokenizer="THUDM/chatglm2-6b")
    
    # 准备一个测试提示 Prompts 列表
    prompts = ["你好，用一句话介绍一下vLLM。"]
    
    # 设置采样参数，例如最多生成50个新tokens，采样温度0.8
    params = SamplingParams(max_tokens=50, temperature=0.8)
    
    # 调用生成函数
    outputs = llm.generate(prompts, params)
    
    # 打印第一个提示对应的生成结果
    print(outputs[0].outputs[0].text)
    
    ```
    
    首次运行时，vLLM 会加载模型权重（可能需要几十秒到几分钟，取决于模型大小和磁盘速度）。随后，它会生成输入提示的续写文本。上述例子中，我们让模型用一句话介绍 vLLM（**提示**以中文给出，因为 ChatGLM2-6B 支持中文）。`llm.generate` 方法会返回一个结果对象列表，其中包含每个输入的输出序列。通过打印输出文本，您可以看到模型给出的回答。如果模型正常产出了合理的文本，恭喜您，本地部署已成功运行！
    
    *提示*: 如果在加载或生成过程中遇到错误，例如显存不足、模型无法下载等，可参考第五节的常见问题部分进行排查。
    
4. **启动本地服务（可选）**：如果您希望将模型部署为一个本地服务供其它程序调用，可以使用 vLLM 的 OpenAI 风格API服务器功能。vLLM 安装后会提供命令行工具，允许一键启动HTTP服务。例如：
    
    ```bash
    vllm serve --model THUDM/chatglm2-6b --port 8000
    
    ```
    
    上述命令将在本地启动一个 OpenAI兼容的服务，默认监听端口8000 ([Quickstart — vLLM](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#:~:text=vLLM%20can%20be%20deployed%20as,You%20can%20specify))。通过这个服务，您可以使用OpenAI API的客户端或curl发送请求到 `http://localhost:8000/v1/chat/completions` 等端点来获取模型回复，就像使用OpenAI的ChatGPT接口一样。对于不支持Chat Completion格式的模型，也可以使用 `/v1/completions` 接口请求普通文本续写。
    
    一旦服务器启动成功，您就完成了模型的本地部署。现在，您的应用程序或其他开发者工具（例如 Postman、curl、Python requests，或LangChain等支持OpenAI接口的库）都可以指向这个本地主机地址，从而调用您的自部署模型。由于 vLLM 遵循 OpenAI API 规范，大多数现有调用代码几乎无需修改即可对接。
    
    *附加*: 如需部署**嵌入向量服务**或**奖励模型服务**，也可以使用类似命令：例如 `vllm serve --model <模型名> --task embed` 来启动一个嵌入服务 ([vllm/examples/online_serving/openai_pooling_client.py at main](https://github.com/vllm-project/vllm/blob/main/examples/online_serving/openai_pooling_client.py#:~:text=vllm%2Fexamples%2Fonline_serving%2Fopenai_pooling_client,up%20the%20server%20in%20vLLM))。这样便可通过`/v1/embeddings`等端点获取句向量表示。
    

以上步骤完成后，您已经在本地成功部署了一个大语言模型，并可以通过编程接口与之交互。根据需求，您可以选择直接在Python中调用（适合批量离线任务），或启动长期运行的服务（适合实时交互或集成到Web应用）。

## 5. 其他有帮助的信息（系统要求、常见问题及优化建议）

在本节，我们汇总一些使用 vLLM 时常见的注意事项和优化建议，帮助初学者顺利部署并获得最佳性能。

- **系统和硬件要求**：vLLM 官方建议在 **Linux** 环境下使用，并支持 x86_64 架构的 CPU/GPU。Python 版本需在 **3.8 到 3.11** 之间 ([Installation - vLLM](https://nm-vllm.readthedocs.io/en/0.5.0/getting_started/installation.html#:~:text=Installation%20,11))。如果使用 GPU，加速库要求 **CUDA 12.1**（vLLM 提供的预编译二进制基于CUDA 12.1） ([Installation - vLLM](https://docs.vllm.ai/en/v0.4.0.post1/getting_started/installation.html#:~:text=Install%20vLLM%20with%20CUDA%2012,PyTorch%20release%20versions%20by%20default))。NVIDIA 显卡应支持 Tensor Core（如RTX20系及以上）以发挥FP16/BF16性能。如果没有 NVIDIA GPU，vLLM 也提供基础的 **CPU 推理支持**，可在仅有CPU的环境运行，但速度会慢很多 ([Installation with CPU - vLLM](https://docs.vllm.ai/en/v0.4.0.post1/getting_started/cpu-installation.html#:~:text=Installation%20with%20CPU%20,data%20types%20FP32%20and%20BF16))。CPU推理目前支持 FP32 和 BF16 等精度 ([Installation with CPU - vLLM](https://docs.vllm.ai/en/v0.4.0.post1/getting_started/cpu-installation.html#:~:text=Installation%20with%20CPU%20,data%20types%20FP32%20and%20BF16))。
- **显存和内存规划**：请根据模型大小选择合适的硬件。一般来说，FP16 精度下模型每参数约占2字节显存。例如7B参数模型需要约14GB显存，13B需要约26GB，70B则需接近140GB显存，单卡难以容纳。这种情况下可以考虑两种方案：其一，使用 vLLM 的多GPU并行支持，将模型拆分到多块显卡上；其二，使用量化模型以降低单卡所需显存。vLLM 支持的 GPTQ、AWQ 等量化格式通常能将显存占用降低一半甚至四分之三，但需要先在外部工具生成相应的量化权重 ([vLLM - Qwen docs](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#:~:text=Serving%20Quantized%20Models%C2%B6,deploy%20AWQ%20and%20GPTQ))。合理利用量化可以让例如13B模型在消费级GPU上运行成为可能。
- **多 GPU 部署**：对于超大模型或提高吞吐，vLLM 可以使用多GPU进行推理。通过启动参数 `-tensor-parallel-size N` 来开启 **张量并行**（模型权重按张量分块到 N 个 GPU），`-pipeline-parallel-size M` 开启 **流水并行**（模型按层切分为 M 段，串联在 M 卡上） ([Distributed Inference and Serving - vLLM](https://docs.vllm.ai/en/latest/serving/distributed_serving.html#:~:text=,size%204))。例如，`vllm serve facebook/opt-13b --tensor-parallel-size 4` 会在4块GPU上加载 OPT-13B 模型 ([Distributed Inference and Serving - vLLM](https://docs.vllm.ai/en/latest/serving/distributed_serving.html#:~:text=,size%204))。使用多GPU可以线性扩展可用显存，总参数量几乎等比扩展。但需要注意，多GPU通信也会带来开销，合理选择并行策略（tensor并行对带宽要求高，pipeline并行对延迟敏感）以获得最佳性能。
- **常见问题排查**：
    - *安装/导入错误*：如果 `pip install vllm` 后 import 仍出错，可能是本地没有匹配CUDA版本的PyTorch。vLLM 安装时通常会自动安装兼容的 torch，例如 torch>=2.0 且CUDA 12版。如果仍有问题，建议先安装与系统CUDA兼容的PyTorch，再安装vLLM。
    - *显存不足*：当出现 *OutOfMemory* 错误时，可尝试减小 `max_tokens`（减少单次生成长度）、降低并发请求数，或切换到量化模型降低显存占用。如果仍不行，只能更换更大显存的GPU或者使用多GPU。
    - *模型下载慢或失败*：从HuggingFace下载模型可能较慢，可提前手动下载或使用国内镜像。确保网络畅通或有足够磁盘空间。对于需要授权的模型（如 LLaMA），请先在 HuggingFace 上接受许可并使用 `huggingface-cli login` 登陆。
- **优化建议**：
    - *并发利用*：充分利用 vLLM 的连续批处理特性。您可以**同时发送多条请求**来提高GPU利用率。vLLM 会智能地将它们打包，提高吞吐。单个长请求可能无法发挥最大性能，多并发通常更高效。
    - *调节生成参数*：在确保需求的前提下，适当减少 `max_tokens`、降低 `temperature`，不仅能减轻计算负担，还能避免模型输出冗长无关的内容，从而提升整体响应速度。
    - *使用 BF16/FP16*：vLLM 默认会尝试使用半精度(如果GPU支持)来加速推理。在保证模型精度的前提下，BF16/FP16 会带来显著的加速和更小的显存占用。确保在加载模型时没有强制fp32即可。
    - *监控和Profiling*：对于追求极致性能，可以使用 NVIDIA 的 profiler 或 vLLM 提供的日志来观察推理的时间开销瓶颈。根据需要调整 `-gpu-memory-utilization` 等高级参数（此参数控制 vLLM 给每个模型分配的显存比例，默认0.9即90%，可以调整以避免内存碎片）。这些微调对于大批量、高并发场景有帮助。

**总结**：vLLM 通过创新的架构设计和诸多优化，实现了本地LLM推理的高效能和易用性 ([Berkeley vLLM：算力减半、吞吐增十倍- lsgxeva - 博客园](https://www.cnblogs.com/lsgxeva/p/18187514#:~:text=com%2Fvllm,%E5%8F%AF%E4%BB%A5%E6%9C%89%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E9%94%AE%E5%92%8C%E5%80%BC%EF%BC%9B%3B%20%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%EF%BC%9B%3B%20%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84))。对于初学者，只需按上述指南安装并加载模型，即可快速在本地体验大语言模型的强大功能。而随着需求增长，vLLM 提供的各种高级特性（如多模型支持、并行扩展等）也能满足更复杂的部署场景。希望本教程能帮助您顺利上手 vLLM，在本地运行属于自己的“大语言模型”！

**参考资料**：

- vLLM 官方文档: *Welcome to vLLM* ([Welcome to vLLM!](https://docs.vllm.ai/en/v0.5.2/#:~:text=vLLM%20is%20flexible%20and%20easy,with%20various%20decoding%20algorithms))、*Quickstart* ([Quickstart — vLLM](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#:~:text=vLLM%20can%20be%20deployed%20as,You%20can%20specify))、*Supported Models* ([Generative Models - vLLM](https://docs.vllm.ai/en/v0.7.1/models/generative_models.html#:~:text=Generative%20Models%20,models%20implement%20the%20VllmModelForTextGeneration)) ([vllm/docs/source/models/pooling_models.md at main - GitHub](https://github.com/vllm-project/vllm/blob/main/docs/source/models/pooling_models.md#:~:text=vllm%2Fdocs%2Fsource%2Fmodels%2Fpooling_models.md%20at%20main%20,class%7D%20~vllm))、*OpenAI-Compatible Server* ([vllm/examples/online_serving/openai_pooling_client.py at main](https://github.com/vllm-project/vllm/blob/main/examples/online_serving/openai_pooling_client.py#:~:text=vllm%2Fexamples%2Fonline_serving%2Fopenai_pooling_client,up%20the%20server%20in%20vLLM)) 等
- vLLM 博客: *Easy, Fast, and Cheap LLM Serving with PagedAttention*（介绍 vLLM 性能） ([Accelerating AI up to 24x with vLLM: Unleashing the True Potential ...](https://www.aiworks.be/accelerating-ai-up-to-24x-with-vllm-unleashing-the-true-potential-of-local-large-language-models/#:~:text=Accelerating%20AI%20up%20to%2024x,Key%20Takeaways))
- 项目讨论: GitHub Issues 中关于 TTS 支持的讨论 ([[Feature]: 【support tts llm like cosyvoice2.0】 · Issue #11964 - GitHub](https://github.com/vllm-project/vllm/issues/11964#:~:text=GitHub%20github,not%20currently%20support%20embedding%20input))
- 技术报告: *Efficient Memory Management for LLM Serving*（arXiv, 提及 vLLM 内存效率） ([An Embarrassingly Simple TTS Framework that Everyone Can Touch](https://arxiv.org/html/2412.08237v1#:~:text=An%20Embarrassingly%20Simple%20TTS%20Framework,requiring%20additional%20deployment))
- 第三方总结: 红帽博客 *什么是vLLM* ([什么是vLLM？ - Red Hat](https://www.redhat.com/zh/topics/aiml/what-vllm#:~:text=%E4%BB%80%E4%B9%88%E6%98%AFvLLM%EF%BC%9F%20,%E7%9A%84%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%88LLM%20%E5%8F%AF%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%89%E6%98%AFHugging%20Face%20Transformers%EF%BC%88))、博客园 *Berkeley vLLM: 算力减半、吞吐增十倍* ([Berkeley vLLM：算力减半、吞吐增十倍- lsgxeva - 博客园](https://www.cnblogs.com/lsgxeva/p/18187514#:~:text=com%2Fvllm,%E5%8F%AF%E4%BB%A5%E6%9C%89%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E9%94%AE%E5%92%8C%E5%80%BC%EF%BC%9B%3B%20%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%EF%BC%9B%3B%20%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84)) 等
- 更多实操教程: DataCamp 博客 *Setting Up vLLM Locally*、Ploomber 博客 *Deploying vLLM: a Step-by-Step Guide* 等。